% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@InCollection{AbowdEtAl2009,
  Title                    = {The {LEHD} Infrastructure Files and the Creation of the {Q}uarterly {W}orkforce {I}ndicators},
  Author                   = {John M. Abowd and Bryce E. Stephens and Lars Vilhuber and Fredrik Andersson and Kevin L. McKinney and Marc Roemer and Simon D. Woodcock},
  Booktitle                = {Timothy Dunne, J. Bradford Jensen, and Mark J. Roberts},
  Publisher                = {University of Chicago Press},
  Year                     = {2009},

  Crossref                 = {DunneJensenRoberts2009},
  File                     = {AbowdStephensVilhuber2005-LEHD-final.pdf:L/LEHD/AbowdStephensVilhuber2005-LEHD-final.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2007.03.13}
}

@TechReport{AbowdEtAl2012,
  Title                    = {Dynamically consistent noise infusion and partially synthetic data as confidentiality protection measures for related time-series},
  Author                   = {John M. Abowd and Kaj Gittings and Kevin L. McKinney and Bryce E. Stephens and Lars Vilhuber and Simon Woodcock},
  Institution              = {Federal Committee on Statistical Methodology},
  Year                     = {2012},
  Month                    = {January},

  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21},
  Url                      = {http://www.fcsm.gov/events/papers2012.html}
}

@Electronic{AbowdVilhuber2010,
  Title                    = {Synthetic Data Server},
  Author                   = {Abowd, John M. and Lars Vilhuber},
  Url                      = {http://www.vrdc.cornell.edu/sds/},
  Year                     = {2010},

  Owner                    = {vilhuber},
  Timestamp                = {2012.05.18}
}

@Article{BenedettoEtAl2007,
  Title                    = {Using Worker Flows in the Analysis of the Firm},
  Author                   = {Gary Benedetto and John Haltiwanger and Julia Lane and Kevin McKinney},
  Journal                  = jbes,
  Year                     = {2007},

  Month                    = jul,
  Number                   = {3},
  Pages                    = {299-313},
  Volume                   = {25},

  Comment                  = {see also LEHD TP-2003-09},
  File                     = {tp-2003-09.pdf:L/LEHD/tp-2003-09.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2007.01.12}
}

@Article{RePEc:taf:japsta:v:39:y:2012:i:2:p:243-265,
  Title                    = {New data dissemination approaches in old {E}urope -- synthetic datasets for a {G}erman establishment survey},
  Author                   = {J\"org Drechsler},
  Journal                  = {Journal of Applied Statistics},
  Year                     = {2012},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {243-265},
  Volume                   = {39},

  Abstract                 = { Disseminating microdata to the public that provide a high level of data utility, while at the same time guaranteeing the confidentiality of the survey respondent is a difficult task. Generating multiply imputed synthetic datasets is an innovative statistical disclosure limitation technique with the potential of enabling the data disseminating agency to achieve this twofold goal. So far, the approach was successfully implemented only for a limited number of datasets in the U.S. In this paper, we present the first successful implementation outside the U.S.: the generation of partially synthetic datasets for an establishment panel survey at the German Institute for Employment Research. We describe the whole evolution of the project: from the early discussions concerning variables at risk to the final synthesis. We also present our disclosure risk evaluations and provide some first results on the data utility of the generated datasets. A variance-inflated imputation model is introduced that incorporates additional variability in the model for records that are not sufficiently protected by the standard synthesis.},
  Url                      = {http://ideas.repec.org/a/taf/japsta/v39y2012i2p243-265.html}
}

@TechReport{RePEc:iab:iabfme:201101_de,
  Title                    = {Synthetische {S}cientific-Use-Files der {W}elle 2007 des {IAB}-{B}etriebspanels},
  Author                   = {Drechsler, J\"org},
  Institution              = {Institute for Employment Research, Nuremberg, Germany},
  Year                     = {2011},
  Month                    = Jan,
  Number                   = {201101\_de},
  Type                     = {FDZ Methodenreport},

  Abstract                 = {\&quot;Providing scientific use files for business surveys is a difficult task. Due to smaller populations, higher sampling rates, and skewed distributions disclosure risks are much higher than for household surveys. Simple measures like coarsening are not sufficient to protect the data. The aim of generating synthetic datasets is to release data that provide a high level of data utility while guaranteeing the confidentiality of the survey respondent. To achieve this, sensitive variables and variables that could be used for re-identification purposes are replaced with multiple imputations. This report gives a short introduction to the topic and discusses some aspects that analysts should keep in mind when using the synthetic datasets. Furthermore, the report describes how valid inferences can be obtained based on the synthetic datasets and provides some first data utility evaluations that indicate the potentials but also the limits of the generated datasets.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Imputationsverfahren; Datenanonymisierung; Datenschutz; Betriebsdatenerfassung; N},
  Url                      = {http://ideas.repec.org/p/iab/iabfme/201101_de.html}
}

@Article{RePEc:spr:alstar:v:95:y:2011:i:1:p:1-26,
  Title                    = {Multiple imputation in practice: a case study using a complex German establishment survey},
  Author                   = {J\"org Drechsler},
  Journal                  = {AStA Advances in Statistical Analysis},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {1-26},
  Volume                   = {95},

  Abstract                 = {No abstract is available for this item.},
  Keywords                 = {Multiple imputation; Fully conditional specification; Model evaluation; Imputation software; IAB Est},
  Url                      = {http://ideas.repec.org/a/spr/alstar/v95y2011i1p1-26.html}
}

@TechReport{RePEc:iab:iabdpa:201006,
  Title                    = {Multiple imputation of missing values in the wave 2007 of the IAB Establishment Panel},
  Author                   = {Drechsler,J\"org},
  Institution              = {Institut f\"ur Arbeitsmarkt- und Berufsforschung (IAB), N\"urnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2010},
  Month                    = Feb,
  Number                   = {201006},
  Type                     = {IAB Discussion Paper},

  Abstract                 = {\&quot;The basic concept of multiple imputation is straightforward and easy to understand, but the application to real data imposes many implementation problems. To define useful imputation models for a dataset that consists of categorical and of continuous variables with distributions that are anything but normal, contains skip patterns and all sorts of logical constraints is a challenging task. In this paper, we review different approaches to handle these problems and illustrate their successful implementation for a complex imputation project at the German Institute for Employment Research (IAB): The imputation of missing values in one wave of the IAB Establishment Panel.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {Missing Data-Technik; Datenqualit√§t; IAB-Betriebspanel; statistische Methode; Imputationsverfahren},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/201006.html}
}

@TechReport{RePEc:iab:iabdpa:200711,
  Title                    = {A new approach for disclosure control in the IAB Establishment Panel : multiple imputation for a better data access},
  Author                   = {Drechsler, J\"org and Dundler, Agnes and Bender, Stefan and R\"assler, Susanne and Zwick, Thomas},
  Institution              = {Institut f\"ur Arbeitsmarkt- und Berufsforschung (IAB), N\"urnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2007},
  Month                    = Feb,
  Number                   = {200711},
  Type                     = {IAB Discussion Paper},

  Abstract                 = {\&quot;For micro-datasets considered for release as scientific or public use files, statistical agencies have to face the dilemma of guaranteeing the confidentiality of survey respondents on the one hand and offering sufficiently detailed data on the other hand. For that reason a variety of methods to guarantee disclosure control is discussed in the literature. In this paper, we present an application of Rubin's (1993) idea to generate synthetic datasets from existing confidential survey data for public release. We use a set of variables from the 1997 wave of the German IAB Establishment Panel and evaluate the quality of the approach by comparing results from an analysis by Zwick (2005) with the original data with the results we achieve for the same analysis run on the dataset after the imputation procedure. The comparison shows that valid inferences can be obtained using the synthetic datasets in this context, while confidentiality is guaranteed for the survey participants.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Datensicherheit; Datenschutz; Datenaufbereitung; Datenanonymisierung; Imputations},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/200711.html}
}

@Article{RePEc:eee:csdana:v:55:y:2011:i:12:p:3232-3243,
  Title                    = {An empirical evaluation of easily implemented, nonparametric methods for generating synthetic datasets},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Computational Statistics \& Data Analysis},
  Year                     = {2011},

  Month                    = {December},
  Number                   = {12},
  Pages                    = {3232-3243},
  Volume                   = {55},

  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@Article{RePEc:bes:jnlasa:v:105:i:492:y:2010:p:1347-1357,
  Title                    = {Sampling With Synthesis: A New Approach for Releasing Public Use Census Microdata},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2010},
  Number                   = {492},
  Pages                    = {1347-1357},
  Volume                   = {105},

  Abstract                 = {No abstract is available for this item.},
  Url                      = {http://ideas.repec.org/a/bes/jnlasa/v105i492y2010p1347-1357.html}
}

@Article{DrechslerReiter2009,
  Title                    = {Disclosure Risk and Data Utility for Partially Synthetic Data: An Empirical Study Using the {G}erman {IAB} {E}stablishment {S}urvey. , Vol. 25, 589-603.},
  Author                   = {Drechsler, J\"org and Reiter, Jerome P.},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2009},

  Month                    = {December},
  Number                   = {12},
  Pages                    = {589-603},
  Volume                   = {25},

  Abstract                 = {When intense redaction is needed to protect the confidentiality of data subjects' identities and sensitive attributes, statistical agencies can use synthetic data approaches. To create synthetic data, the agency replaces identifying or sensitive values with draws from statistical models estimated from the confidential data. Many agencies are reluctant to implement this idea because (i) the quality of the generated data depends strongly on the quality of the underlying models, and (ii) developing effective synthesis models can be a labor-intensive and difficult task. Recently, there have been suggestions that agencies use nonparametric methods from the machine learning literature to generate synthetic data. These methods can estimate non-linear relationships that might otherwise be missed and can be run with minimal tuning, thus considerably reducing burdens on the agency. Four synthesizers based on machine learning algorithms-classification and regression trees, bagging, random forests, and support vector machines-are evaluated in terms of their potential to preserve analytical validity while reducing disclosure risks. The evaluation is based on a repeated sampling simulation with a subset of the 2002 Uganda census public use sample data. The simulation suggests that synthesizers based on regression trees can result in synthetic datasets that provide reliable estimates and low disclosure risks, and that these synthesizers can be implemented easily by statistical agencies.},
  Keywords                 = { Census Confidentiality Disclosure Imputation Microdata Synthetic},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://ideas.repec.org/a/eee/csdana/v55y2011i12p3232-3243.html}
}

@TechReport{DrechslerVilhuber2013,
  Title                    = {Replicating the {S}ynthetic {LBD} with {G}erman Establishment Data},
  Author                   = {Drechsler, J\"org and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},

  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@TechReport{Fort2013,
  Title                    = {Applying a Consistent Industry Classification Across Time},
  Author                   = {Teresa Fort},
  Institution              = {Center for Economic Studies},
  Year                     = {2013},

  Owner                    = {fort}
}

@TechReport{RePEc:iab:iabfme:201006_en,
  Title                    = {Using worker flows in the analysis of establishment turnover : {E}vidence from {G}erman administrative data},
  Author                   = {Hethey, Tanja and Schmieder, Johannes F.},
  Institution              = {Institute for Employment Research, Nuremberg, Germany},
  Year                     = {2010},
  Month                    = Aug,
  Number                   = {201006\_en},
  Type                     = {FDZ Methodenreport},

  Abstract                 = {\&quot;Economists have long been interested in the determinants and components of job creation and destruction. In many countries administrative datasets provide an excellent source for detailed analysis on a fine and disaggregate level. However, administrative datasets are not without problems: restructuring and relabeling of firms is often poorly measured and can potentially create large biases. We provide evidence of the extent of this bias and provide a new solution to deal with it using the German Establishment History Panel (BHP). While previous research has relied on the first and last appearance of the establishment identifier (EID) to identify openings and closings, we improve on this approach using a new dataset containing all worker flows between establishments in Germany. This allows us to credibly identify establishment births and deaths from 1975 to 2004. We show that the misclassification bias of using only the EID is very severe: Only about 35 to 40 percent of new and disappearing EIDs with more than 3 employees correspond unambiguously to real establishment entries and exits. Among larger establishments misclassification is even more common. We show that many new establishment IDs appear to be 'Spin-Offs' and these have become increasingly more common over time. We then demonstrate that using only EID entries and exits may dramatically overstate, by as much as 100 percent, the role of establishment turnover for job creation and destruction. Furthermore correcting job creation and destruction measures for spurious EID entries and exits reduces these measures and aligns them closer with the business cycle.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {labour turnover; },
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://ideas.repec.org/p/iab/iabfme/201006_en.html}
}

@TechReport{MirandaJarmin2002,
  Title                    = {The {L}ongitudinal {B}usiness {D}atabase},
  Author                   = {Ron Jarmin and Javier Miranda},
  Institution              = {U.S. Census Bureau, Center for Economic Studies},
  Year                     = {2002},
  Number                   = {CES-WP-02-17},
  Type                     = {Discussion Paper},

  Abstract                 = {The LBD is a research dataset constructed at the Census Bureau's Center for Economic Studies. The LBD is an establishment based file created by linking the annual snapshot files from Census Bureau's Business Register over time. It contains high quality longitudinal establishment linkages. Firm level linkages are currently under development at CES. The LBD contains several basic data items such as firm ownership, location, industry, payroll and employment.},
  File                     = {MirandaJarmin2002.pdf:M/MirandaJarmin2002.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2009.09.25}
}

@TechReport{KinneyEtAl2013,
  Title                    = {{SynLBD}: providing firm characteristics on synthetic establishment data},
  Author                   = {Kinney, S. K. and Reiter, J.},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},

  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@Article{KinneyEtAl2011,
  Title                    = {Towards Unrestricted Public Use Business Microdata: The {S}ynthetic {L}ongitudinal {B}usiness {D}atabase},
  Author                   = {Satkartar K. Kinney and Jerome P. Reiter and Arnold P. Reznek and Javier Miranda and Ron S. Jarmin and John M. Abowd},
  Journal                  = {International Statistical Review},
  Year                     = {2011},

  Month                    = {December},
  Number                   = {3},
  Pages                    = {362-384},
  Volume                   = {79},

  Abstract                 = {In most countries, national statistical agencies do not release establishment-level business microdata, because doing so represents too large a risk to establishments\' confidentiality. One approach with the potential for overcoming these risks is to release synthetic data; that is, the released establishment data are simulated from statistical models designed to mimic the distributions of the underlying real microdata. In this article, we describe an application of this strategy to create a public use file for the Longitudinal Business Database, an annual economic census of establishments in the United States comprising more than 20 million records dating back to 1976. The U.S. Bureau of the Census and the Internal Revenue Service recently approved the release of these synthetic microdata for public use, making the synthetic Longitudinal Business Database the first-ever business microdata set publicly released in the United States. We describe how we created the synthetic data, evaluated analytical validity, and assessed disclosure risk.},
  File                     = {KinneyEtAl2011.pdf:K/KinneyEtAl2011.pdf:PDF},
  Owner                    = {vilhuber},
  Timestamp                = {2012.05.18},
  Url                      = {http://ideas.repec.org/a/bla/istatr/v79y2011i3p362-384.html}
}

@Article{little93,
  Title                    = {Statistical Analysis of Masked Data},
  Author                   = {Roderick J.A. Little},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {407-426},
  Volume                   = {9},

  Owner                    = {John Abowd},
  Timestamp                = {2008.04.29}
}

@Article{Ashwin2008,
  Title                    = {Privacy: {T}heory meets practice on the map},
  Author                   = {Ashwin Machanavajjhala and Daniel Kifer and John M. Abowd and Johannes Gehrke and Lars Vilhuber},
  Journal                  = {International Conference on Data Engineering (ICDE)},
  Year                     = {2008},

  Owner                    = {vilhuber},
  Timestamp                = {2012.05.21}
}

@TechReport{MirandaVilhuber2013,
  Title                    = {Looking back on three years of {S}ynthetic {LBD} {B}eta},
  Author                   = {Miranda, Javier and Vilhuber, Lars},
  Institution              = {World Statistics Conference},
  Year                     = {2013},
  Type                     = {Presentation},

  Owner                    = {vilhuber},
  Timestamp                = {2013.06.16}
}

@Article{reiter03syntheticInference,
  Title                    = {Multiple imputation for statistical disclosure limitation},
  Author                   = {T.E. Raghunathan and J.P. Reiter and D.B. Rubin},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2003},
  Pages                    = {1--16},
  Volume                   = {19},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiterCartMI,
  Title                    = {Using CART to generate partially synthetic public use microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of Official Statistics},
  Year                     = {2005},
  Pages                    = {441--462},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Voluem                   = {21}
}

@Article{reiterDisclosureRisk,
  Title                    = {Estimating risks of identification disclosure for microdata},
  Author                   = {J.P. Reiter},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2005},
  Pages                    = {1103 -- 1113},
  Volume                   = { 100},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{reiter03partiallySyntheticInference,
  Title                    = {Inference for Partially Synthetic, Public Use Microdata Sets},
  Author                   = {J.P. Reiter},
  Journal                  = {Survey Methodology},
  Year                     = {2003},
  Number                   = {2},
  Pages                    = {181--188},
  Volume                   = {29},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{Reiter2008,
  Title                    = {Multiple imputation when records used for imputation are not used or disseminated for analysis},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Biometrika},
  Year                     = {2008},
  Number                   = {4},
  Pages                    = {933-946},
  Volume                   = {95},

  Abstract                 = {When some of the records used to estimate the imputation models in multiple imputation are not used or available for analysis, the usual multiple imputation variance estimator has positive bias. We present an alternative approach that enables unbiased estimation of variances and, hence, calibrated inferences in such contexts. First, using all records, the imputer samples m values of the parameters of the imputation model. Second, for each parameter draw, the imputer simulates the missing values for all records n times. From these mn completed datasets, the imputer can analyse or disseminate the appropriate subset of records. We develop methods for interval estimation and significance testing for this approach. Methods are presented in the context of multiple imputation for measurement error.},
  Doi                      = {10.1093/biomet/asn042},
  Eprint                   = {http://biomet.oxfordjournals.org/content/95/4/933.full.pdf+html},
  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12},
  Url                      = {http://biomet.oxfordjournals.org/content/95/4/933.abstract}
}

@Article{reiter04newApproaches,
  Title                    = {New approaches to data dissemination: A glimpse into the future (?)},
  Author                   = {J. P. Reiter},
  Journal                  = {Chance},
  Year                     = {2004},
  Number                   = {3},
  Pages                    = {12--16},
  Volume                   = {17},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Article{Reiter2003b,
  Title                    = {Model Diagnostics for Remote Access Regression Servers},
  Author                   = {Reiter, Jerome P.},
  Journal                  = {Statistics and Computing},
  Year                     = {2003},
  Note                     = {10.1023/A:1025623108012},
  Pages                    = {371-380},
  Volume                   = {13},

  Abstract                 = {To protect public-use microdata, one approach is not to allow users access to the microdata. Instead, users submit analyses to a remote computer that reports back basic output from the fitted model, such as coefficients and standard errors. To be most useful, this remote server also should provide some way for users to check the fit of their models, without disclosing actual data values. This paper discusses regression diagnostics for remote servers. The proposal is to release synthetic diagnostics?i.e. simulated values of residuals and dependent and independent variables?constructed to mimic the relationships among the real-data residuals and independent variables. Using simulations, it is shown that the proposed synthetic diagnostics can reveal model inadequacies without substantial increase in the risk of disclosures. This approach also can be used to develop remote server diagnostics for generalized linear models.},
  Affiliation              = {Institute of Statistics and Decision Sciences Duke University Box 90251 Durham NC 27708 USA},
  Doi                      = {10.1023/A:1025623108012},
  File                     = {Reiter2003b.pdf:R/Reiter2003b.pdf:PDF},
  ISSN                     = {0960-3174},
  Issue                    = {4},
  Keyword                  = {Mathematics and Statistics},
  Owner                    = {vilhuber},
  Publisher                = {Springer Netherlands},
  Timestamp                = {2012.10.18},
  Url                      = {http://dx.doi.org/10.1023/A:1025623108012}
}

@TechReport{RePEc:iab:iabdpa:200720,
  Title                    = {Releasing multiply-imputed synthetic data generated in two stages to protect confidentiality},
  Author                   = {Reiter, Jerome P. and Drechsler, J√∂rg},
  Institution              = {Institut f√ºr Arbeitsmarkt- und Berufsforschung (IAB), N√ºrnberg [Institute for Employment Research, Nuremberg, Germany]},
  Year                     = {2007},
  Month                    = Jun,
  Number                   = {200720},
  Type                     = {IAB Discussion Paper},

  Abstract                 = {\&quot;To protect the cofidentiality of survey respondents' identities and sensitive attributes, statistical agencies can release data in which cofidential values are replaced with multiple imputations. These are called synthetic data. We propose a two-stage approach to generating synthetic data that enables agencies to release different numbers of imputations for different variables. Generation in two stages can reduce computational burdens, decrease disclosure risk, and increase inferential accuracy relative to generation in one stage. We present methods for obtaining inferences from such data. We describe the application of two stage synthesis to creating a public use file for a German business database.\&quot; (Author's abstract, IAB-Doku) ((en))},
  Keywords                 = {IAB-Betriebspanel; Datenaufbereitung; Datenanonymisierung; Datenschutz; angewandte Statistik; statis},
  Url                      = {http://ideas.repec.org/p/iab/iabdpa/200720.html}
}

@TechReport{RePEc:iaw:iawdip:66,
  Title                    = {Remote Access ‚Äì Eine Welt ohne Mikrodaten??},
  Author                   = {Gerd Ronning and Philipp Bleninger and J√∂rg Drechsler and Christopher G√ºrke},
  Institution              = {Institut f√ºr Angewandte Wirtschaftsforschung (IAW)},
  Year                     = {2010},
  Month                    = Jun,
  Number                   = {66},
  Type                     = {IAW Discussion Papers},

  Abstract                 = {Use of microdata is severely hampered in many areas of research. This is in particular true for data from statistical offices. One way to circumvent this problem is to anonymize the data such that both confidentiality is guaranteed and informational content of the data is not to much distorted by the anonymization procedure. However many researchers prefer the use of 'original' data. Therefore in recent years remote access/execution ('Fernrechnen') has become quite popular where the original micro data are used in the statistical analysis but are not available to the researchers. Clearly, this alternative takes more time since program files have to be sent to the statistical office. However, the euphoria for this approach has cooled down a bit since it has become apparent that here also problems of condentiality exist. Most obvious is the fact that residuals cannot be provided. See, for example, Gomatam et al. (2005). However, there are very different kinds of 'disclosures' which are discussed in the paper. The paper also draws attention to the use of saturated models which bear the risk of reproducing confidential tabular data. Analysis of variance is the relevant tool in reproducing magnitude tables whereas the corresponding micro-econometric models can be used to reproduce frequency tables: Logit models give the results in case of a nominal variable and Poisson regression is the approach in case of count data.We also shortly discuss possible disclosure risk in the standard multivariate procedures (factor analysis, principal components, cluster analysis and multidimensional scaling). It is clear from the many examples given in the paper that the remote access/execution option will ask for a large amount of statistical expertise in the statistical office in order to check for disclosure risk. Additionally, there will be a tendency not to provide statistical results to the researcher if critical variables such as region or sector are demanded as regressors in the pr},
  Keywords                 = {Minimum wage; regulation; employment; meta-analysis},
  Url                      = {http://ideas.repec.org/p/iaw/iawdip/66.html}
}

@Article{rubin93,
  Title                    = {Discussion of Statistical Disclosure Limitation},
  Author                   = {Donald B. Rubin},
  Journal                  = {Journal of Official Statistics},
  Year                     = {1993},
  Number                   = {2},
  Pages                    = {461-468},
  Volume                   = {9},

  Owner                    = {John Abowd},
  Timestamp                = {2008.04.29}
}

@TechReport{SynLBD20,
  Title                    = {Synthetic {LBD} {Beta} Version 2.0},
  Author                   = {{U.S. Census Bureau}},
  Institution              = {{U.S. Census Bureau} and Cornell University, Synthetic Data Server [distributor]},
  Year                     = {2011},

  Address                  = {Washington,DC and Ithaca, NY, USA},
  Type                     = {[Computer file]},

  Abstract                 = {The Synthetic LBD Beta Data Product (SynLBD) is an experimental data product produced by the U.S. Census Bureau in collaboration with Duke University, Cornell University, the National Institute of Statistical Sciences (NISS), the Internal Revenue Service (IRS) and the National Science Foundation (NSF). The purpose of the SynLBD is to provide users with access to a longitudinal business data product that can be used outside of a secure Census Bureau facility. The Census Bureau created version 2 of the SynLBD by synthesizing information on establishments' employment and payroll, establishments' birth and death years, and industrial classification. The Census Disclosure Review Board and their counterparts at IRS have reviewed the content of the file, and allowed the release of these data for public use.},
  HowPublished             = {Computer file},
  Organization             = {Cornell University, Synthetic Data Server [distributor]},
  Owner                    = {vilhuber},
  Timestamp                = {2013.06.10},
  Url                      = {http://www2.vrdc.cornell.edu/news/data/lbd-synthetic-data/}
}

@TechReport{Winkler2007,
  Title                    = {Examples of Easy-to-implement, Widely Used Methods of Masking for which Analytic Properties are not Justified.},
  Author                   = {Winkler, William E.},
  Institution              = {U.S. Census Bureau},
  Year                     = {2007},
  Number                   = {2007-21},
  Type                     = {Research Report Series},

  Owner                    = {vilhuber},
  Timestamp                = {2013.04.12}
}

@Proceedings{DBLP:conf/psd/2012,
  Title                    = {Privacy in Statistical Databases - UNESCO Chair in Data Privacy, International Conference, PSD 2012, Palermo, Italy, September 26-28, 2012. Proceedings},
  Year                     = {2012},
  Editor                   = {Josep Domingo-Ferrer and Ilenia Tinnirello},
  Publisher                = {Springer},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {7556},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Booktitle                = {Privacy in Statistical Databases},
  Doi                      = {10.1007/978-3-642-33627-0},
  ISBN                     = {978-3-642-33626-3},
  Owner                    = {vilhuber},
  Timestamp                = {2012.09.26},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-33627-0}
}

@Book{DunneJensenRoberts2009,
  Title                    = {Producer Dynamics: New Evidence from Micro Data},
  Editor                   = {Timothy Dunne and J. Brad Jensen and Mark J. Roberts},
  Publisher                = {The University of Chicago Press for the National Bureau of Economic Research},
  Year                     = {2009},

  Owner                    = {vilhuber},
  Timestamp                = {2006.03.07}
}


@report{BDS2,
author = {John Haltiwanger and Ron Jarmin and Javier Miranda},
title = {Jobs Created from Business Startups in the United States},
type = {BDS Brief},
institution = {Ewing Marion Kauffman Foundation},
year = {2008},
number = {1},
url = {https://www.census.gov/ces/pdf/BDS_StatBrief1_Jobs_Created.pdf},
urldate = {2014-05-08},
}

@techreport{NBERw16300,
 title = "Who Creates Jobs? Small vs. Large vs. Young",
 author = "John C. Haltiwanger and Ron S. Jarmin and Javier Miranda",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "16300",
 year = "2010",
 month = "August",
 URL = "http://www.nber.org/papers/w16300",
 abstract = {The view that small businesses create the most jobs remains appealing to policymakers and small business advocates. Using data from the Census Bureau Business Dynamics Statistics and Longitudinal Business Database, we explore the many issues at the core of this ongoing debate. We find that the relationship between firm size and employment growth is sensitive to these issues. However, our main finding is that once we control for firm age there is no systematic relationship between firm size and growth. Our findings highlight the important role of business startups and young businesses in U.S. job creation.},
}
@article{DrechslerReiter2012,
author = {Drechsler, J. and Reiter, J. P.},
title = {Combining synthetic data with subsampling to create public use microdata files for large scale surveys},
journaltitle = {Survey Methodology},
year = {2012},
volume = {38},
number = {1},
month = {June},
pages = {73-79},
}

@TechReport{RePEc:cen:wpaper:13-19,
  author={Joseph W. Sakshaug and Trivellore E. Raghunathan},
  title={{Synthetic Data For Small Area Estimation In The American Community Survey}},
  year=2013,
  month=Apr,
  institution={Center for Economic Studies, U.S. Census Bureau},
  type={Working Papers},
  url={http://ideas.repec.org/p/cen/wpaper/13-19.html},
  number={13-19},
  abstract={Small area estimates provide a critical source of information used to study local populations. Statistical agencies regularly collect data from small areas but are prevented from releasing detailed geographical identifiers in public-use data sets due to disclosure concerns. Alternative data dissemination methods used in practice include releasing summary/aggregate tables, suppressing detailed geographic information in public-use data sets, and accessing restricted data via Research Data Centers. This research examines an alternative method for disseminating microdata that contains more geographical details than are currently being released in public-use data files. Specifically, the method replaces the observed survey values with imputed, or synthetic, values simulated from a hierarchical Bayesian model. Confidentiality protection is enhanced because no actual values are released. The method is demonstrated using restricted data from the 2005-2009 American Community Survey. The analytic validity of the synthetic data is assessed by comparing small area estimates obtained from the synthetic data with those obtained from the observed data.},
  keywords={},
}

@report{Rodriguez2007,
author = {Rolando Rodr\'iguez},
title = {Synthetic Data Disclosure Control for American Community Survey Group Quarters},
type = {presentation},
institution = {Joint Statistical Meetings},
year = {2007},
}

@Article{HolanEtAl2010,
  Title                    = {Bayesian Multiscale Multiple Imputation With Implications for Data Confidentiality},
  Author                   = {Holan, Scott H. and Toth, Daniell and Ferreira, Marco A. R. and Karr, Alan F.},
  Journal                  = {Journal of the American Statistical Association},
  Year                     = {2010},
  Number                   = {490},
  Pages                    = {564-577},
  Volume                   = {105},

  Abstract                 = { Many scientific, sociological, and economic applications present data that are collected on multiple scales of resolution. One particular form of multiscale data arises when data are aggregated across different scales both longitudinally and by economic sector. Frequently, such datasets experience missing observations in a manner that they can be accurately imputed, while respecting the constraints imposed by the multiscale nature of the data, using the method we propose known as Bayesian multiscale multiple imputation. Our approach couples dynamic linear models with a novel imputation step based on singular normal distribution theory. Although our method is of independent interest, one important implication of such methodology is its potential effect on confidential databases protected by means of cell suppression. In order to demonstrate the proposed methodology and to assess the effectiveness of disclosure practices in longitudinal databases, we conduct a large-scale empirical study using the U.S. Bureau of Labor Statistics Quarterly Census of Employment and Wages (QCEW). During the course of our empirical investigation it is determined that several of the predicted cells are within 1% accuracy, thus causing potential concerns for data confidentiality. },
  Doi                      = {10.1198/jasa.2009.ap08629},
  Eprint                   = {http://dx.doi.org/10.1198/jasa.2009.ap08629},
  Url                      = {http://dx.doi.org/10.1198/jasa.2009.ap08629}
}

@article{tas2006,
author = {A. F. KARR and C. N. KOHNEN and A. OGANIAN  and  J. P. REITER  and A. P. SANIL},
title = {A Framework for Evaluating the Utility of Data Altered to Protect Confidentiality},
journaltitle = {The American Statistician},
year = {2006},
volume = {60},
number = {3},
pages = {1-9},
doi = {10.1198/000313006X124640},
}
@PHDTHESIS{Gittings2009thesis,
  author = {Robert Kaj Gittings},
  title = {Essays in labor economics and synthetic data methods},
  school = {Cornell University},
  year = {2009},
  type = {Ph.D.},
  abstract = {Three topics are investigated in these chapters: the causes and consequences
        of lateral job mobility within firms, the impact of incentives on
        human behavior in the context of capital punishment and deterrence,
        and the development of new synthetic data methods for confidentiality
        protection of public use data. The extent and importance of lateral
        mobility is not well-established in economics and Chapter 1 contributes
        new and important findings to the literature. Using a panel of more
        than 500 firms and 48,000 white-collar workers, I find relatively
        high rates of lateral mobility, that this mobility is statistically
        different from other transitions, and that the compensation growth
        associated with lateral mobility is economically meaningful. I also
        investigate the relationships between worker performance, compensation
        growth and job mobility. Even when controlling for productivity differences,
        significant earnings growth occurs directly through the change in
        jobs. The results provide some evidence that the observed lateral
        mobility may be the result of job rotation. In light of continued
        debate of whether capital punishment deters crime, Chapter 2 revisits
        my previous work on this issue and shows that the deterrence results
        hold under alternative measurements of key variables, multiple statistical
        specifications and subsets of the data. Chapter 3 develops methodology
        that solves the need for statistical agencies to suppress certain
        data items because releasing those cells to the public yields a risk
        of exposing someone's personal information. I show that the synthetic
        data adequately protect the confidential data and are superior in
        terms of its analytical validity.},
  owner = {vilhuber},
  timestamp = {2012.04.15}
}

@ARTICLE{BradleyEtAl2014,
   author = {{Bradley}, J.~R. and {Holan}, S.~H. and {Wikle}, C.~K.},
    title = "{Mixed Effects Modeling for Areal Data that Exhibit Multivariate-Spatio-Temporal 
    Dependencies}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1407.7479},
 primaryClass = "stat.ME",
 keywords = {Statistics - Methodology},
     year = 2014,
    month = jul,
   adsurl = {http://adsabs.harvard.edu/abs/2014arXiv1407.7479B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{CNSTATBusinessDynamics,
  title={Understanding Business Dynamics: An Integrated Data System for America's Future},
  author={{Panel on Measuring Business Formation, Dynamics, and Performance:} and  John 
  Haltiwanger and Lisa M. Lynch and Christopher Mackie},
  isbn={9780309104920},
  url={http://www.nap.edu/openbook.php?record_id=11844},
  year={2007},
  publisher={National Research Council, The National Academies Press}
}


@comment{jabref-meta: selector_review:}

@comment{jabref-meta: selector_publisher:}

@comment{jabref-meta: selector_author:}

@comment{jabref-meta: selector_journal:}

@comment{jabref-meta: selector_keywords:}

